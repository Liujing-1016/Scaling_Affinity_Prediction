{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# !pip install faiss-gpu\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smiles_to_ecfp(smiles_list, radius=2, n_bits=2048):\n",
    "    \"\"\"Convert a list of SMILES strings to ECFP fingerprints.\"\"\"\n",
    "    ecfp_features = []\n",
    "    for smi in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "            ecfp_features.append(list(fp))\n",
    "        else:\n",
    "            ecfp_features.append([0] * n_bits)  \n",
    "    return np.array(ecfp_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load data & ECFP4 generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(train_files):\n",
    "    \"\"\"Load multiple CSV files and convert them into ECFP features.\"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    for f in train_files:\n",
    "        print(f\"Loading {f}...\")\n",
    "        chunk = pd.read_csv(f, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        # Generate ECFP4\n",
    "        smiles_list = chunk.iloc[:, 0].tolist()\n",
    "        ecfp_features = smiles_to_ecfp(smiles_list, n_bits=2048)  \n",
    "        other_features = chunk.iloc[:, 1:-6143].values.astype(np.float32)  \n",
    "        X_chunk = np.hstack((ecfp_features, other_features))  \n",
    "        \n",
    "        y_chunk = chunk.iloc[:, -6143:].values.astype(np.float32)\n",
    "        \n",
    "        X_list.append(X_chunk)\n",
    "        y_list.append(y_chunk)\n",
    "    \n",
    "    X = np.vstack(X_list)\n",
    "    y = np.vstack(y_list)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train FAISS KNN index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_faiss_knn(X_train, y_train, index_file=\"faiss_knn.index\", y_file=\"faiss_y_train.pkl\"):\n",
    "    \"\"\"Create and store a FAISS index.\"\"\"\n",
    "    d = X_train.shape[1] \n",
    "    index = faiss.IndexFlatL2(d)  # Use L2 (Euclidean) distance\n",
    "    \n",
    "    print(\"Training FAISS index...\")\n",
    "    index.add(X_train)  # Add training data to the FAISS index\n",
    "\n",
    "    faiss.write_index(index, index_file)\n",
    "    joblib.dump(y_train, y_file)\n",
    "    print(\"FAISS index saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Use KNN for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_test, k=5, index_file=\"faiss_knn.index\", y_file=\"faiss_y_train.pkl\"):\n",
    "    \"\"\"Perform prediction using FAISS KNN.\"\"\"\n",
    "    index = faiss.read_index(index_file)  \n",
    "    y_train = joblib.load(y_file)  \n",
    "    \n",
    "    # Find the k nearest neighbors\n",
    "    distances, indices = index.search(X_test, k)  \n",
    "    y_pred = np.mean(y_train[indices], axis=1)  \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../uncoverted_dataset_csv/train_chunks_csv/train_chunk1.csv...\n",
      "Loading ../uncoverted_dataset_csv/train_chunks_csv/train_chunk2.csv...\n",
      "Loading ../uncoverted_dataset_csv/train_chunks_csv/train_chunk3.csv...\n",
      "Loading ../uncoverted_dataset_csv/train_chunks_csv/train_chunk4.csv...\n",
      "Loading ../uncoverted_dataset_csv/train_chunks_csv/train_chunk5.csv...\n",
      "Loading ../uncoverted_dataset_csv/train_chunks_csv/train_chunk6.csv...\n",
      "Loading ../uncoverted_dataset_csv/train_chunks_csv/train_chunk7.csv...\n",
      "Loading ../uncoverted_dataset_csv/train_chunks_csv/train_chunk8.csv...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_files = [\n",
    "        \"../uncoverted_dataset_csv/train_chunks_csv/train_chunk1.csv\",\n",
    "        \"../uncoverted_dataset_csv/train_chunks_csv/train_chunk2.csv\",\n",
    "        \"../uncoverted_dataset_csv/train_chunks_csv/train_chunk3.csv\",\n",
    "        \"../uncoverted_dataset_csv/train_chunks_csv/train_chunk4.csv\",\n",
    "        \"../uncoverted_dataset_csv/train_chunks_csv/train_chunk5.csv\",\n",
    "        \"../uncoverted_dataset_csv/train_chunks_csv/train_chunk6.csv\",\n",
    "        \"../uncoverted_dataset_csv/train_chunks_csv/train_chunk7.csv\",\n",
    "        \"../uncoverted_dataset_csv/train_chunks_csv/train_chunk8.csv\"\n",
    "    ]\n",
    "\n",
    "    X_train, y_train = load_dataset(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_train = X_train.reshape(-1, X_train.shape[1])\n",
    "\n",
    "# Create FAISS index\n",
    "d = X_train.shape[1]  # Feature dimension\n",
    "index = faiss.IndexFlatL2(d)  # Use Euclidean distance\n",
    "\n",
    "index.add(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FAISS index...\n",
      "FAISS index saved.\n"
     ]
    }
   ],
   "source": [
    "train_faiss_knn(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Load test set & get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_set(test_file):\n",
    "    chunk = pd.read_csv(test_file, encoding=\"utf-8-sig\")\n",
    "    \n",
    "   \n",
    "    smiles_list = chunk.iloc[:, 0].tolist()\n",
    "    ecfp_features = smiles_to_ecfp(smiles_list, n_bits=2048)\n",
    "    other_features = chunk.iloc[:, 1:-6143].values.astype(np.float32) \n",
    "    \n",
    "    # Combine ECFP features with other features\n",
    "    X_test = np.hstack((ecfp_features, other_features))\n",
    "\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (19358, 2048)\n"
     ]
    }
   ],
   "source": [
    "x_test2 = load_test_set(\"test_smiles12.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test2 = x_test2.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = knn_predict(x_test2, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9  ...  6133  6134  6135  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "\n",
      "   6136  6137  6138  6139  6140  6141  6142  \n",
      "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 6143 columns]\n"
     ]
    }
   ],
   "source": [
    "df_pred2 = pd.DataFrame(y_pred2)\n",
    "df_pred2.columns = [f\"{i}\" for i in range(df_pred2.shape[1])]\n",
    "print(df_pred2.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles2 = pd.read_csv(\"test_smiles12.csv\")\n",
    "df_pred2.insert(0, \"smiles\", smiles2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles    0    1    2    3    4  \\\n",
      "0  CN1CCC(n2cnc(-c3ccc(F)cc3)c2-c2ccnc(Nc3ccncc3)...  0.0  0.0  0.0  0.0  0.0   \n",
      "1        Cc1cc(C)nc(-n2nc(C)cc2NC(=O)CN2CCC(C)CC2)n1  0.0  0.0  0.0  0.0  0.0   \n",
      "2     C#CCSC[C@H](NC(=O)c1cc(OC)c(OC)c(OC)c1)C(=O)OC  0.0  0.0  0.0  0.0  0.0   \n",
      "3  COc1c(NC(=O)/C(=N/O)c2ccc(OCCN3CCOCC3)c3ccccc2...  0.0  0.0  0.0  0.0  0.0   \n",
      "4               O=C(O)c1ccccc1Nc1cc(F)cc(C(F)(F)F)c1  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "     5    6    7    8  ...  6133  6134  6135  6136  6137  6138  6139  6140  \\\n",
      "0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "3  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   6141  6142  \n",
      "0   0.0   0.0  \n",
      "1   0.0   0.0  \n",
      "2   0.0   0.0  \n",
      "3   0.0   0.0  \n",
      "4   0.0   0.0  \n",
      "\n",
      "[5 rows x 6144 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_pred2.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_pred(test_file):\n",
    "    smiles1 = pd.read_csv(test_file)\n",
    "    x_test = load_test_set(test_file)  \n",
    "    x_test = x_test.astype(np.float32)\n",
    "\n",
    "\n",
    "    pred = knn_predict(x_test, k=5)\n",
    "    df_pred= pd.DataFrame(pred)\n",
    "    df_pred.columns = [f\"{i}\" for i in range(df_pred.shape[1])]\n",
    "    df_pred.insert(0, \"smiles\", smiles1)  \n",
    "\n",
    "    return df_pred\n",
    "\n",
    "    \n",
    "    \n",
    "df_pred4 = get_knn_pred(\"test_smiles22.csv\")\n",
    "df_pred4.to_csv(\"test4_pred_KNN.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
